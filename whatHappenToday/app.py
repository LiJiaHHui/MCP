import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv

# --- 1. ç¯å¢ƒè®¾ç½®ä¸åŠ è½½ ---
# ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ (OPENAI_API_KEY)
load_dotenv()

def check_api_key():
    """æ£€æŸ¥ OpenAI API Key æ˜¯å¦å·²è®¾ç½®"""
    if not os.getenv("OPENAI_API_KEY"):
        st.error("OpenAI API Key æœªè®¾ç½®ï¼è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª .env æ–‡ä»¶ï¼Œå¹¶å†™å…¥ OPENAI_API_KEY='sk-...'")
        st.stop()


# --- 2. æ ¸å¿ƒ AI é€»è¾‘ ---
def generate_summary(dialogue_text: str) -> str:
    """
    ä½¿ç”¨ LangChain å’Œå¤§è¯­è¨€æ¨¡å‹åˆ†æå¹¶æ€»ç»“å¯¹è¯ã€‚

    Args:
        dialogue_text: åŒ…å«å¯¹è¯è®°å½•çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        ç”± AI ç”Ÿæˆçš„ Markdown æ ¼å¼çš„æ€»ç»“æŠ¥å‘Šã€‚
    """
    # --- Prompt Template (æç¤ºè¯æ¨¡æ¿) ---
    # è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„çµé­‚ã€‚æˆ‘ä»¬é€šè¿‡è¿™ä¸ªæ¨¡æ¿ç²¾ç¡®åœ°æŒ‡å¯¼ LLM å¦‚ä½•è¡ŒåŠ¨ã€‚
    # ä¸€ä¸ªå¥½çš„æ¨¡æ¿æ˜¯é«˜è´¨é‡è¾“å‡ºçš„ä¿è¯ã€‚
    prompt_template = """
    ä½ æ˜¯ä¸€åé¡¶çº§çš„é¡¹ç›®ç»ç†å’Œèµ„æ·±æŠ€æœ¯æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸“ä¸šã€æ·±å…¥åœ°åˆ†æä»¥ä¸‹å›¢é˜Ÿå¯¹è¯è®°å½•ã€‚
    è¯·æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä»¥ç»“æ„åŒ–ã€æ—¶é—´çº¿æ¸…æ™°çš„æ ¼å¼ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„äº‹ä»¶å¤ç›˜æŠ¥å‘Šã€‚

    ä½ çš„æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼Œå¹¶ä¸¥æ ¼éµå¾ªè¦æ±‚ï¼š

    ### 1. äº‹ä»¶æ¦‚è¿°
    ç”¨ä¸€å¥è¯é«˜åº¦æ¦‚æ‹¬æ•´ä¸ªäº‹ä»¶çš„æ ¸å¿ƒå†…å®¹ã€‚

    ### 2. é—®é¢˜æ—¶é—´çº¿ (Timeline)
    ä¸¥æ ¼æŒ‰ç…§æ—¶é—´é¡ºåºï¼Œç®€æ´åˆ—å‡ºæ¯ä¸ªå…³é”®ä¸”é‡è¦èŠ‚ç‚¹çš„äººç‰©å’Œè¡Œä¸ºã€‚
    - **10:00 AM (å°æ):** å‘ç°å¹¶æŠ¥å‘Šäº†ä»€ä¹ˆé—®é¢˜ï¼ˆé™„ä¸Šå…³é”®æ—¥å¿—ï¼‰ã€‚
    - **10:02~10:05 AM (è€ç‹):** æå‡ºäº†åˆæ­¥çš„çŒœæµ‹ã€‚ç»™å‡ºäº†å…·ä½“çš„æ’æŸ¥æŒ‡ä»¤ï¼ˆå¦‚ `top` å‘½ä»¤ï¼‰ã€‚
    - **10:08 AM (å°æ):** æ‰§è¡ŒæŒ‡ä»¤å¹¶åé¦ˆäº†ä»€ä¹ˆå…³é”®ä¿¡æ¯ï¼ˆå¦‚ `top` æˆªå›¾å†…å®¹ï¼‰ã€‚
    - **...ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°é—®é¢˜è§£å†³ã€‚**

    ### 3. æ ¹æœ¬åŸå›  (Root Cause Analysis)
    ä¸€å¥è¯æ¸…æ™°ã€å‡†ç¡®åœ°æŒ‡å‡ºå¯¼è‡´é—®é¢˜çš„æŠ€æœ¯æ ¹æœ¬åŸå› ã€‚

    ### 4. è§£å†³æ–¹æ¡ˆ
    æè¿°æœ€ç»ˆé‡‡ç”¨çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆä»¥åŠç”±è°å®Œæˆã€‚

    ### 5. æ€»ç»“ä¸åæ€
    ä¸€å¥è¯æç‚¼å‡ºæœ¬æ¬¡äº‹ä»¶çš„å…³é”®æ•™è®­ï¼Œä»¥åŠæœªæ¥å¯ä»¥å¦‚ä½•æ”¹è¿›ä»¥é¿å…ç±»ä¼¼é—®é¢˜ã€‚
    è¯·ç¡®ä¿ä½ çš„æŠ¥å‘Šå®Œå…¨åŸºäºä»¥ä¸‹æä¾›çš„å¯¹è¯åŸæ–‡ï¼Œä¿æŒå®¢è§‚ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¯¹è¯ä¸­æœªæåŠçš„ä¿¡æ¯ã€‚
    è¯·ä½¿ç”¨ Markdown æ ¼å¼è¿›è¡Œè¾“å‡ºï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°ç¾è§‚ã€‚ä¸å‡ºç°ä»£ç å—ï¼Œç”¨ç©ºæ ¼+æ–œä½“ä»£æ›¿ã€‚

    ---
    ã€å¯¹è¯è®°å½•åŸæ–‡ã€‘
    {dialogue}
    ---
    """

    # åˆ›å»º Prompt æ¨¡æ¿å®ä¾‹
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # åˆå§‹åŒ– LLM æ¨¡å‹
    # temperature=0.1 ä½¿å¾—è¾“å‡ºæ›´ç¨³å®šã€å¯å¤ç°
    llm = ChatOpenAI(temperature=0.1, model_name="gpt-4o-mini")

    # åˆ›å»º LLMChainï¼Œå°† Prompt å’Œ LLM "é“¾æ¥" åœ¨ä¸€èµ·
    chain = LLMChain(llm=llm, prompt=prompt)

    # æ‰§è¡Œé“¾ï¼Œå¹¶è·å–ç»“æœ
    try:
        response = chain.invoke({"dialogue": dialogue_text})
        # è¿”å›ç»“æœä¸­çš„æ–‡æœ¬éƒ¨åˆ†
        return response.get('text', 'æŠ±æ­‰ï¼Œæœªèƒ½ç”ŸæˆæŠ¥å‘Šã€‚')
    except Exception as e:
        st.error(f"è°ƒç”¨ API æ—¶å‡ºé”™: {e}")
        return None


# --- 3. Streamlit ç”¨æˆ·ç•Œé¢ ---
def main():
    st.set_page_config(page_title="What Happened Today", page_icon="â“", layout="wide")
    check_api_key()

    # st.title("What Happened Today")
    # st.caption("")
    # å¤§æ ‡é¢˜
    st.markdown('<p class="main-title">â“What Happened Today</p>', unsafe_allow_html=True)

    # æè¿°
    st.markdown('<p class="description">Extract structured report from any chat â€” just by copy</p>', unsafe_allow_html=True)
    # st.markdown("""
    # **Hiï¼** è¿™æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿæ¢³ç†æ€»ç»“å·¥ä½œå¯¹è¯çš„MCPé¡¹ç›®ã€‚
    # å®ƒèƒ½å°†éç»“æ„åŒ–çš„å·¥ä½œå¯¹è¯ï¼ˆå¦‚ä¸€æ¬¡çº¿ä¸Šé—®é¢˜æ’æŸ¥çš„èŠå¤©è®°å½•ï¼‰è‡ªåŠ¨æ¢³ç†æˆä¸€ä»½å¸¦æ—¶é—´çº¿çš„ã€ç»“æ„æ¸…æ™°çš„å¤ç›˜æŠ¥å‘Šã€‚ğŸ˜€
    # """)

    # å·¦å³å¸ƒå±€
    col1, col2 = st.columns(2)



    with col1:
        st.subheader("ğŸ“‹ Original conversation")
        try:
            with open("conversation_example.txt", "r", encoding="utf-8") as f:
                example_text = f.read()
        except FileNotFoundError:
            example_text = "ç¤ºä¾‹æ–‡ä»¶ (conversation_example.txt) æœªæ‰¾åˆ°ã€‚"

        dialogue_input = st.text_area(
            "è¯·å°†å¯¹è¯è®°å½•ç²˜è´´äºæ­¤:",
            value=example_text,
            height=700,
            label_visibility="collapsed"
        )

    with col2:
        st.subheader("âœ¨ AI-generated review reports")

        if 'summary' not in st.session_state:
            st.session_state.summary = "empty..."

        if st.button("Start Now", type="primary", use_container_width=True):
            if not dialogue_input.strip():
                st.warning("Please input chat~")
            else:
                with st.spinner("AI is in the process of deep analysis, please wait..."):
                    summary_output = generate_summary(dialogue_input)
                    if summary_output:
                        st.session_state.summary = summary_output
                    else:
                        st.session_state.summary = "Report generation failed, check the API Key or network."

        # ä½¿ç”¨ Markdown ç»„ä»¶å±•ç¤ºæŠ¥å‘Šï¼Œå¹¶è®¾ç½®è¾¹æ¡†å’Œå†…è¾¹è·
        st.markdown(f'{st.session_state.summary}', unsafe_allow_html=True)
        st.markdown("""
            <style>
            /* ä¸»æ ‡é¢˜æ ·å¼ */
            .main-title {
                font-size: 56px !important;
                font-weight: bold;
                text-align: center;
                color: #000; /* ç™½è‰²å­—ä½“ */
                padding-top: 40px;
            }
            /* å‰¯æ ‡é¢˜/æè¿°æ ·å¼ */
            .description {
                font-size: 20px !important;
                text-align: center;
                color: #B0B0B0; /* ç°è‰²å­—ä½“ */
                padding-bottom: 40px;
            }
            /* Streamlit ä¸»ä½“èƒŒæ™¯è‰² */
            .stApp {
                background-color:  #eeebe8; /* é»‘è‰²èƒŒæ™¯ */
            }
            /* Tab æ ‡ç­¾æ ·å¼ */
            .stTabs [data-baseweb="tab-list"] {
                justify-content: center;
            }
            .stTabs [data-baseweb="tab"] {
                height: 50px;
                white-space: pre-wrap;
                background-color: #1a1a1a;
                border-radius: 8px;
                margin: 0 5px;
                color: #f2ddcc; /* Tab æœªé€‰ä¸­æ—¶å­—ä½“é¢œè‰² */
                
            }
            .stTabs [aria-selected="true"] {
                background-color: #333333;
                color: #f2ddcc; /* Tab é€‰ä¸­æ—¶å­—ä½“é¢œè‰² */
            }
            .st-emotion-cache-bfgnao p{
                font-weight: 650;
                font-size: 1.0625rem;
                line-height: 1.625rem;
                font-variation-settings: "opsz" 40, "wght" 650;
                font-synthesis: none;
            }
            </style>
            """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
