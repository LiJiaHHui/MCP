import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv

# --- 1. ç¯å¢ƒè®¾ç½®ä¸åŠ è½½ ---
# ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ (OPENAI_API_KEY)
load_dotenv()


def check_api_key():
    """æ£€æŸ¥ OpenAI API Key æ˜¯å¦å·²è®¾ç½®"""
    if not os.getenv("OPENAI_API_KEY"):
        st.error("OpenAI API Key æœªè®¾ç½®ï¼è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª .env æ–‡ä»¶ï¼Œå¹¶å†™å…¥ OPENAI_API_KEY='sk-...'")
        st.stop()


# --- 2. æ ¸å¿ƒ AI é€»è¾‘ ---
def generate_summary(dialogue_text: str) -> str:
    """
    ä½¿ç”¨ LangChain å’Œå¤§è¯­è¨€æ¨¡å‹åˆ†æå¹¶æ€»ç»“å¯¹è¯ã€‚

    Args:
        dialogue_text: åŒ…å«å¯¹è¯è®°å½•çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        ç”± AI ç”Ÿæˆçš„ Markdown æ ¼å¼çš„æ€»ç»“æŠ¥å‘Šã€‚
    """

    # --- Prompt Template (æç¤ºè¯æ¨¡æ¿) ---
    # è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„çµé­‚ã€‚æˆ‘ä»¬é€šè¿‡è¿™ä¸ªæ¨¡æ¿ç²¾ç¡®åœ°æŒ‡å¯¼ LLM å¦‚ä½•è¡ŒåŠ¨ã€‚
    # ä¸€ä¸ªå¥½çš„æ¨¡æ¿æ˜¯é«˜è´¨é‡è¾“å‡ºçš„ä¿è¯ã€‚
    prompt_template = """
    ä½ æ˜¯ä¸€åé¡¶çº§çš„é¡¹ç›®ç»ç†å’Œèµ„æ·±æŠ€æœ¯æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸“ä¸šã€æ·±å…¥åœ°åˆ†æä»¥ä¸‹å›¢é˜Ÿå¯¹è¯è®°å½•ã€‚
    è¯·æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä»¥ç»“æ„åŒ–ã€æ—¶é—´çº¿æ¸…æ™°çš„æ ¼å¼ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„äº‹ä»¶å¤ç›˜æŠ¥å‘Šã€‚

    ä½ çš„æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼Œå¹¶ä¸¥æ ¼éµå¾ªè¦æ±‚ï¼š

    ### 1. äº‹ä»¶æ¦‚è¿°
    ç”¨ä¸€å¥è¯é«˜åº¦æ¦‚æ‹¬æ•´ä¸ªäº‹ä»¶çš„æ ¸å¿ƒå†…å®¹ã€‚

    ### 2. é—®é¢˜æ—¶é—´çº¿ (Timeline)
    ä¸¥æ ¼æŒ‰ç…§æ—¶é—´é¡ºåºï¼Œè¯¦ç»†åˆ—å‡ºæ¯ä¸ªå…³é”®èŠ‚ç‚¹çš„äººç‰©ã€è¡Œä¸ºå’Œå‘ç°ã€‚
    - **10:00 AM (å°æ):** å‘ç°å¹¶æŠ¥å‘Šäº†ä»€ä¹ˆé—®é¢˜ï¼ˆé™„ä¸Šå…³é”®æ—¥å¿—ï¼‰ã€‚
    - **10:02 AM (è€ç‹):** æå‡ºäº†åˆæ­¥çš„çŒœæµ‹ã€‚
    - **10:05 AM (è€ç‹):** ç»™å‡ºäº†å…·ä½“çš„æ’æŸ¥æŒ‡ä»¤ï¼ˆå¦‚ `top` å‘½ä»¤ï¼‰ã€‚
    - **10:08 AM (å°æ):** æ‰§è¡ŒæŒ‡ä»¤å¹¶åé¦ˆäº†ä»€ä¹ˆå…³é”®ä¿¡æ¯ï¼ˆå¦‚ `top` æˆªå›¾å†…å®¹ï¼‰ã€‚
    - **...ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°é—®é¢˜è§£å†³ã€‚**

    ### 3. é—®é¢˜æ ¹æœ¬åŸå›  (Root Cause Analysis)
    æ¸…æ™°ã€å‡†ç¡®åœ°æŒ‡å‡ºå¯¼è‡´é—®é¢˜çš„æŠ€æœ¯æ ¹æœ¬åŸå› ã€‚

    ### 4. è§£å†³æ–¹æ¡ˆä¸æ‰§è¡Œ
    æè¿°æœ€ç»ˆé‡‡ç”¨çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆä»¥åŠç”±è°å®Œæˆã€‚

    ### 5. æ€»ç»“ä¸åæ€
    æç‚¼å‡ºæœ¬æ¬¡äº‹ä»¶çš„å…³é”®æ•™è®­ï¼Œä»¥åŠæœªæ¥å¯ä»¥å¦‚ä½•æ”¹è¿›ä»¥é¿å…ç±»ä¼¼é—®é¢˜ã€‚

    è¯·ç¡®ä¿ä½ çš„æŠ¥å‘Šå®Œå…¨åŸºäºä»¥ä¸‹æä¾›çš„å¯¹è¯åŸæ–‡ï¼Œä¿æŒå®¢è§‚ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¯¹è¯ä¸­æœªæåŠçš„ä¿¡æ¯ã€‚
    è¯·ä½¿ç”¨ Markdown æ ¼å¼è¿›è¡Œè¾“å‡ºï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°ç¾è§‚ã€‚

    ---
    ã€å¯¹è¯è®°å½•åŸæ–‡ã€‘
    {dialogue}
    ---
    """

    # åˆ›å»º Prompt æ¨¡æ¿å®ä¾‹
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # åˆå§‹åŒ– LLM æ¨¡å‹
    # temperature=0.1 ä½¿å¾—è¾“å‡ºæ›´ç¨³å®šã€å¯å¤ç°
    llm = ChatOpenAI(temperature=0.1, model_name="gpt-4o-mini")

    # åˆ›å»º LLMChainï¼Œå°† Prompt å’Œ LLM "é“¾æ¥" åœ¨ä¸€èµ·
    chain = LLMChain(llm=llm, prompt=prompt)

    # æ‰§è¡Œé“¾ï¼Œå¹¶è·å–ç»“æœ
    try:
        response = chain.invoke({"dialogue": dialogue_text})
        # è¿”å›ç»“æœä¸­çš„æ–‡æœ¬éƒ¨åˆ†
        return response.get('text', 'æŠ±æ­‰ï¼Œæœªèƒ½ç”ŸæˆæŠ¥å‘Šã€‚')
    except Exception as e:
        st.error(f"è°ƒç”¨ API æ—¶å‡ºé”™: {e}")
        return None


# --- 3. Streamlit ç”¨æˆ·ç•Œé¢ ---
def main():
    st.set_page_config(page_title="æ™ºèƒ½å·¥ä½œæ—¥å¿—åˆ†æå™¨", page_icon="ğŸ¤–", layout="wide")

    check_api_key()

    st.title("æ™ºèƒ½å·¥ä½œæ—¥å¿—åˆ†æå™¨")
    st.caption("?What Happened Today - Powered by LangChain & Streamlit")

    st.markdown("""
    **Hiï¼** è¿™æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿå±•ç¤ºçš„ MCP é¡¹ç›®ã€‚
    å®ƒèƒ½å°†éç»“æ„åŒ–çš„å·¥ä½œå¯¹è¯ï¼ˆå¦‚ä¸€æ¬¡çº¿ä¸Šé—®é¢˜æ’æŸ¥çš„èŠå¤©è®°å½•ï¼‰è‡ªåŠ¨æ¢³ç†æˆä¸€ä»½å¸¦æ—¶é—´çº¿çš„ã€ç»“æ„æ¸…æ™°çš„å¤ç›˜æŠ¥å‘Šã€‚
    """)

    # å·¦å³å¸ƒå±€
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“‹ åŸå§‹å¯¹è¯è®°å½•")
        try:
            with open("conversation_example.txt", "r", encoding="utf-8") as f:
                example_text = f.read()
        except FileNotFoundError:
            example_text = "ç¤ºä¾‹æ–‡ä»¶ (conversation_example.txt) æœªæ‰¾åˆ°ã€‚"

        dialogue_input = st.text_area(
            "è¯·å°†å¯¹è¯è®°å½•ç²˜è´´äºæ­¤:",
            value=example_text,
            height=500,
            label_visibility="collapsed"
        )

    with col2:
        st.subheader("âœ¨ AI ç”Ÿæˆçš„å¤ç›˜æŠ¥å‘Š")

        if 'summary' not in st.session_state:
            st.session_state.summary = "ç‚¹å‡»å·¦ä¾§æŒ‰é’®å¼€å§‹ç”ŸæˆæŠ¥å‘Š..."

        if st.button("ğŸš€ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š", type="primary", use_container_width=True):
            if not dialogue_input.strip():
                st.warning("è¯·è¾“å…¥å¯¹è¯å†…å®¹ï¼")
            else:
                with st.spinner("AI æ­£åœ¨æ·±åº¦åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                    summary_output = generate_summary(dialogue_input)
                    if summary_output:
                        st.session_state.summary = summary_output
                    else:
                        st.session_state.summary = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œã€‚"

        # ä½¿ç”¨ Markdown ç»„ä»¶å±•ç¤ºæŠ¥å‘Šï¼Œå¹¶è®¾ç½®è¾¹æ¡†å’Œå†…è¾¹è·
        st.markdown(f'{st.session_state.summary}', unsafe_allow_html=True)

        if __name__ == "__main__":
            main()
